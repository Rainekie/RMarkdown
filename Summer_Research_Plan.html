<!DOCTYPE html>
<html>
<head>
<title>Summer Research Plan.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="summer-research-plan">Summer Research Plan</h1>
<h2 id="topics">Topics</h2>
<ul>
<li>
<p>Temporal Window of Integration for an audio-tactile situation</p>
<ul>
<li>
<p>Comparison between SJ task and Bayesian inference of hierarchical signal detection theory</p>
</li>
<li>
<p>Comparison of task difference and personality using EEG</p>
</li>
<li>
<p>Social interaction of TWI behavior</p>
</li>
</ul>
</li>
<li>
<p>Neuro-feedback game (RIT - U of Iowa)</p>
</li>
<li>
<p>Cultural difference of spatial selective attention (with Sungyoung)</p>
<ul>
<li>
<p>spatial coordinate response measure (CRM) test</p>
</li>
<li>
<p>spatial oddball paradigm</p>
</li>
</ul>
</li>
<li>
<p>Room acoustics estimation using LiDAR sensor (with Sungyoung)</p>
</li>
</ul>
<hr>
<h2 id="temporal-window-of-integration-for-an-audio-tactile-situation">Temporal Window of Integration for an audio-tactile situation</h2>
<h3 id="introduction">- Introduction</h3>
<p>Multisensory information of digital content at appropriate timing offers player's immersion and presence. Especially for sound-related tactile sensations, multimodal stimuli can be generated from direct contact or close proximity to a target object. Coupled with unique features which each sensory naturally has, a multimodal stimulus enables us to perceive details of a target object's distance and texture. Therefore, the presentation of a sound-related vibrotactile stimulus in the digital content could improve user experience, which may lead to enhanced immersion into a given context.</p>
<p>The perception of time and, in particular, synchrony between human senses is not straightforward because there is no dedicated sense organ that registers time on an absolute scale. Moreover, to perceive the synchrony of multimodal information, the brain has to deal with differences in physical and neural transmission time. However, even though some physical parameters such as presenting timings or spatial distances are slightly different, our brain can integrate and perceive multisensory information as occurring at the same timing if a temporal difference is within a short time window.</p>
<p>This time window to create perceptual temporal synchrony calls Temporal Window of Integration (hereinafter called TWI). There are two typical methods to measure TWI; Simultaneous Judgement task (SJ task) and Temporal Order Judgement task (TOJ task). The TOJ and SJ tasks have been used more or less interchangeably, however, several studies report that these methods may bring different TWI values because SJ and TOJ tasks may be subject to different kinds of response biases. Due to the nature of the experiment, these tasks are difficult to control these biases, and thus, to the best knowledge of the authors, there is no straightforward and reasonable solution to appropriately measure TWI.</p>
<p>We hereby attempted to measure TWI based on the discriminability $d^{\prime}$ of Signal Detection Theory (SDT) to reduce these concerns above as much as possible. SDT can independently obtain the sensibility of the participant's signal discriminability $d^{\prime}$ and the judgment criterion $\beta$. Therefore, the parameter $d^{\prime}$ calculated by SDT, which be derived independently from participants' criteria, has a possibility to derive a more decent TWI. The conventional SDT has limitations on data distribution, SDT using Bayesian inference can solve this problem. Consequently, the SDT using Bayesian inference can both avoid the effect of response bias and overcome the assumption of a conventional SDT.</p>
<h3 id="research-purposes-and-goal-until-this-summer">- Research purposes and goal until this summer</h3>
<ol>
<li>Comparison between SJ task and Bayesian inference of hierarchical signal detection theory</li>
</ol>
<p>Theoretically, Bayesian inference of signal detection theory can overcome the disadvantages of conventional TWI measurement theory. However, we not sure how much these disadvantages could overcome and how close this proposed theory could be decent TWI.</p>
<p>Therefore, we will　investigate the method difference  comparing these measurements and try to extract the effect of response biases which in SJ task inherently has. There are several previous studies which compare the difference of SJ and TOJ task, thus, we will use these comparison techniques.</p>
<p>Moreover, we will attempt to investigate individual or group's TWI trend using a hierarchical model. To employ this method, we will consider the trend and the reason more systematically.
\ why we can see the trend using the hierarchical model?</p>
<p>I will do data collection this may and write a journal paper by the end of summer.</p>
<ol start="2">
<li>Comparison of task difference and personality using EEG</li>
</ol>
<p>We attempt to see the individual difference using specific ERPs. Basharat et al. reported that, compared to younger adults, older adults showed a sustained higher auditory N1 ERP amplitude response across SOAs, suggestive of broader response properties from an extended temporal binding window. We will use this research paradigm to investigate the reason for TWI group differences in terms of personality.</p>
<p>I will do data collection this August and write a journal paper by the end of this year.</p>
<ol start="3">
<li>Social interaction of TWI behavior</li>
</ol>
<p>We will think about the TWI behavior changing with social interaction. In our previous researches, we will focus on only intermodality interaction (such as one person's audio-tactile synchrony). However, interpersonal influence should exist. For example, when two users collaboratively playing games and music, they have to meet their internal timing to accomplish their task. At this time, their TWIs will be calibrated to align the timing and behavior.</p>
<p>We will attempt to investigate this phenomenon using the method of Embodied cognitive science. There are several studies to investigate interpersonal synchrony calibration of brain oscillation. We will try to see the phenomenon and the cause using these methods, EEG or fMRI.</p>
<p>I will write the proposal by the end of August, and continue this research at OIST.</p>
<hr>
<h2 id="neuro-feedback-game---purpose-and-goal">Neuro-feedback game - purpose and goal</h2>
<p>Collaborative research project between RIT  and the University of Iowa (Prof. Inyong Choi). It is a series of experiments on developing an ear-training system for people who have hearing difficulties employing a neuroscientific approach. In this term, we will focus on the two topics; 1. the data quality differences between OpenBCI which is a self-made commercial EEG device, and BioSemi, a research-purpose EEG device. 2. making the prototype of a Neuro-feedback game using Unity and Matlab.</p>
<p>We already finish some data analysis about the data quality and reliability such as jitter between trigger and EEGs, a trend of maximum potential, and epoch canceling method based on its potential, overall ERP quality. I am not sure we can write a small paper about this topic.</p>
<p>We will focus on making a user interface using Unity by end of this May, and think about the best and easiest way to conducting the experiment.</p>
<hr>
<h2 id="cultural-difference-of-spatial-selective-attention---abstract-and-goal">Cultural difference of spatial selective attention - abstract and goal</h2>
<h3 id="spatial-coordinate-response-measure-crm-test">spatial coordinate response measure (CRM) test</h3>
<p>I already made the experiment. I will collect the subjective data this May.</p>
<h3 id="spatial-oddball-paradigm">spatial oddball paradigm</h3>
<p>I already made the prototype of the experiment, yet, seems we need to discuss and improve more the experimental design to be able to obtain participants' spatial selective attention.</p>
<hr>
<h2 id="room-acoustics-estimation-using-lidar-sensor">Room acoustics estimation using LiDAR sensor</h2>
<p>LiDAR is a method for determining ranges (variable distance) by targeting an object with a laser and measuring the time for the reflected light to return to the receiver. Lidar can also be used to make digital 3-D representations of areas on the earth's surface and ocean bottom, due to differences in laser return times, and by varying laser wavelengths.</p>
<p>Recently, this sensor has been installed on iPhone and can be used easily. Most AR applications estimate the surface and user's position using this sensor, however, it seems there are not applications to use it for acoustic estimation.</p>
<p>Therefore, we will attempt to estimate a room impulse response using this technology. We have already made a prototype of an IR estimation program using LiDAR-based point cloud data. We will improve the room shape estimation method more, and compare the differences of actual room IR and the estimated data.</p>
<hr>
<h2 id="schedule">schedule</h2>
<p><img src="summerShedule.png" alt="summer schedule"></p>

</body>
</html>
